# ì¸ê³µì§€ëŠ¥ í•™ìŠµ ë¡œë“œë§µ (1ì¼ ì†ì„± ê³¼ì •)

## ğŸ“š ì¶œì²˜
- [DataCamp - How to Learn AI from Scratch](https://www.datacamp.com/blog/how-to-learn-ai)
- [Udacity - AI Roadmap for Beginners](https://www.udacity.com/blog/2025/05/how-to-learn-ai-in-2025-a-roadmap-for-beginners-and-developers.html)
- [AI Engineer Roadmap](https://roadmap.sh/ai-engineer)
- [Medium - AI Roadmap 2025](https://medium.com/madhukarkumar/a-developers-roadmap-to-getting-started-with-ai-in-2025-f3f000ef6770)
- [GitHub - Complete AI ML Roadmap](https://github.com/aadi1011/AI-ML-Roadmap-from-scratch)

## ğŸ¯ í•™ìŠµ ëª©í‘œ
- AIì˜ í•µì‹¬ ê°œë…ê³¼ ë¶„ì•¼ ì´í•´
- ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ì˜ ê¸°ë³¸ ì›ë¦¬ íŒŒì•…
- ë”¥ëŸ¬ë‹ê³¼ ì‹ ê²½ë§ì˜ êµ¬ì¡° í•™ìŠµ
- ì‹¤ë¬´ì—ì„œ ì‚¬ìš©í•˜ëŠ” AI ë„êµ¬ì™€ í”„ë ˆì„ì›Œí¬ ì²´í—˜

## ğŸ“‹ 1ì¼ í•™ìŠµ ê³„íš (8ì‹œê°„)

### 1ë‹¨ê³„: AI ê¸°ì´ˆì™€ ê°œë… (2ì‹œê°„)
#### 1.1 ì¸ê³µì§€ëŠ¥ ê°œìš” (45ë¶„)
- **AIì˜ ì •ì˜ì™€ ì—­ì‚¬**
  - 1950ë…„ëŒ€: íŠœë§ í…ŒìŠ¤íŠ¸
  - 1980ë…„ëŒ€: ì „ë¬¸ê°€ ì‹œìŠ¤í…œ
  - 2010ë…„ëŒ€: ë”¥ëŸ¬ë‹ í˜ì‹ 
  - 2020ë…„ëŒ€: ìƒì„±í˜• AI ì‹œëŒ€

- **AIì˜ ë¶„ë¥˜**
```
ì¸ê³µì§€ëŠ¥ (AI)
â”œâ”€â”€ ì•½í•œ AI (Narrow AI)
â”‚   â”œâ”€â”€ ë¨¸ì‹ ëŸ¬ë‹ (ML)
â”‚   â”‚   â”œâ”€â”€ ì§€ë„í•™ìŠµ (Supervised)
â”‚   â”‚   â”œâ”€â”€ ë¹„ì§€ë„í•™ìŠµ (Unsupervised)
â”‚   â”‚   â””â”€â”€ ê°•í™”í•™ìŠµ (Reinforcement)
â”‚   â””â”€â”€ ë”¥ëŸ¬ë‹ (Deep Learning)
â”‚       â”œâ”€â”€ CNN (ì»¨ë³¼ë£¨ì…˜)
â”‚       â”œâ”€â”€ RNN (ìˆœí™˜ì‹ ê²½ë§)
â”‚       â””â”€â”€ Transformer
â””â”€â”€ ê°•í•œ AI (AGI) - ë¯¸ë˜ ëª©í‘œ
```

#### 1.2 ì£¼ìš” AI ë¶„ì•¼ (45ë¶„)
- **ì»´í“¨í„° ë¹„ì „**: ì´ë¯¸ì§€ ì¸ì‹, ê°ì²´ íƒì§€
- **ìì—°ì–´ ì²˜ë¦¬**: í…ìŠ¤íŠ¸ ë¶„ì„, ì–¸ì–´ ëª¨ë¸
- **ìŒì„± ì¸ì‹**: STT, TTS ê¸°ìˆ 
- **ë¡œë³´í‹±ìŠ¤**: ììœ¨ ë¡œë´‡, ì œì–´ ì‹œìŠ¤í…œ
- **ì¶”ì²œ ì‹œìŠ¤í…œ**: ê°œì¸í™”, í˜‘ì—… í•„í„°ë§

#### 1.3 AI ìœ¤ë¦¬ì™€ í˜„ì‹¤ (30ë¶„)
- AI í¸í–¥ì„±ê³¼ ê³µì •ì„±
- ê°œì¸ì •ë³´ ë³´í˜¸ ì´ìŠˆ
- AIì˜ ì‚¬íšŒì  ì˜í–¥
- ì¼ìë¦¬ ë³€í™”ì™€ ëŒ€ì‘ ë°©ì•ˆ

### 2ë‹¨ê³„: ë¨¸ì‹ ëŸ¬ë‹ ê¸°ì´ˆ (2ì‹œê°„)
#### 2.1 ë¨¸ì‹ ëŸ¬ë‹ ê°œë… (1ì‹œê°„)
- **í•™ìŠµ íŒ¨ëŸ¬ë‹¤ì„**
```python
# ì§€ë„í•™ìŠµ ì˜ˆì œ (ë¶„ë¥˜)
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.datasets import iris

# ë°ì´í„° ë¡œë“œ
X, y = iris.data, iris.target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# ëª¨ë¸ í•™ìŠµ
model = LogisticRegression()
model.fit(X_train, y_train)

# ì˜ˆì¸¡
predictions = model.predict(X_test)
```

- **í‰ê°€ ì§€í‘œ**
  - ì •í™•ë„ (Accuracy)
  - ì •ë°€ë„ (Precision)
  - ì¬í˜„ìœ¨ (Recall)
  - F1-Score

#### 2.2 ì£¼ìš” ì•Œê³ ë¦¬ì¦˜ (1ì‹œê°„)
- **ì„ í˜• íšŒê·€**: ì—°ì†ê°’ ì˜ˆì¸¡
- **ë¡œì§€ìŠ¤í‹± íšŒê·€**: ë¶„ë¥˜ ë¬¸ì œ
- **ê²°ì • íŠ¸ë¦¬**: ê·œì¹™ ê¸°ë°˜ í•™ìŠµ
- **ëœë¤ í¬ë ˆìŠ¤íŠ¸**: ì•™ìƒë¸” í•™ìŠµ
- **SVM**: ì„œí¬íŠ¸ ë²¡í„° ë¨¸ì‹ 
- **K-Means**: í´ëŸ¬ìŠ¤í„°ë§

### 3ë‹¨ê³„: ë”¥ëŸ¬ë‹ê³¼ ì‹ ê²½ë§ (2ì‹œê°„)
#### 3.1 ì‹ ê²½ë§ ê¸°ì´ˆ (1ì‹œê°„)
```python
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

# ê°„ë‹¨í•œ ì‹ ê²½ë§ êµ¬ì¡°
model = Sequential([
    Dense(128, activation='relu', input_shape=(784,)),  # ì…ë ¥ì¸µ
    Dense(64, activation='relu'),                       # ì€ë‹‰ì¸µ
    Dense(10, activation='softmax')                     # ì¶œë ¥ì¸µ
])

# ëª¨ë¸ ì»´íŒŒì¼
model.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['accuracy']
)
```

- **í™œì„±í™” í•¨ìˆ˜**: ReLU, Sigmoid, Tanh
- **ì†ì‹¤ í•¨ìˆ˜**: MSE, Cross-Entropy
- **ìµœì í™”**: Gradient Descent, Adam
- **ì—­ì „íŒŒ**: ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ ë©”ì»¤ë‹ˆì¦˜

#### 3.2 ë”¥ëŸ¬ë‹ ì•„í‚¤í…ì²˜ (1ì‹œê°„)
- **CNN (Convolutional Neural Networks)**
```python
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten

cnn_model = Sequential([
    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),
    MaxPooling2D((2, 2)),
    Conv2D(64, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),
    Flatten(),
    Dense(64, activation='relu'),
    Dense(10, activation='softmax')
])
```

- **RNN/LSTM**: ì‹œê³„ì—´ ë°ì´í„° ì²˜ë¦¬
- **Transformer**: í˜„ëŒ€ NLPì˜ í•µì‹¬
- **GAN**: ìƒì„±ì  ì ëŒ€ ì‹ ê²½ë§

### 4ë‹¨ê³„: ì‹¤ë¬´ ë„êµ¬ì™€ ì‹¤ìŠµ (2ì‹œê°„)
#### 4.1 ì£¼ìš” í”„ë ˆì„ì›Œí¬ (1ì‹œê°„)
- **TensorFlow/Keras**
```python
# TensorFlow ì˜ˆì œ - ì´ë¯¸ì§€ ë¶„ë¥˜
import tensorflow as tf

# ë°ì´í„°ì…‹ ë¡œë“œ
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()

# ë°ì´í„° ì „ì²˜ë¦¬
x_train = x_train.reshape(-1, 28, 28, 1).astype('float32') / 255.0
x_test = x_test.reshape(-1, 28, 28, 1).astype('float32') / 255.0

# ëª¨ë¸ í•™ìŠµ
model.fit(x_train, y_train, epochs=5, validation_data=(x_test, y_test))
```

- **PyTorch**
```python
import torch
import torch.nn as nn

class SimpleNet(nn.Module):
    def __init__(self):
        super(SimpleNet, self).__init__()
        self.fc1 = nn.Linear(784, 128)
        self.fc2 = nn.Linear(128, 10)
        
    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = self.fc2(x)
        return x

model = SimpleNet()
```

- **Scikit-learn**: ì „í†µì  ë¨¸ì‹ ëŸ¬ë‹
- **Hugging Face**: NLP ëª¨ë¸ ë¼ì´ë¸ŒëŸ¬ë¦¬

#### 4.2 ì‹¤ìŠµ í”„ë¡œì íŠ¸ (1ì‹œê°„)
**í”„ë¡œì íŠ¸ 1: ì†ê¸€ì”¨ ìˆ«ì ì¸ì‹**
```python
# ì™„ì „í•œ MNIST ë¶„ë¥˜ê¸°
import tensorflow as tf
from tensorflow.keras import layers, models
import matplotlib.pyplot as plt

# ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()
train_images = train_images.reshape((60000, 28, 28, 1)).astype('float32') / 255
test_images = test_images.reshape((10000, 28, 28, 1)).astype('float32') / 255

# CNN ëª¨ë¸ êµ¬ì„±
model = models.Sequential([
    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.Flatten(),
    layers.Dense(64, activation='relu'),
    layers.Dense(10, activation='softmax')
])

# ì»´íŒŒì¼ ë° í›ˆë ¨
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

history = model.fit(train_images, train_labels, epochs=5,
                    validation_data=(test_images, test_labels))

# ê²°ê³¼ ì‹œê°í™”
plt.plot(history.history['accuracy'], label='accuracy')
plt.plot(history.history['val_accuracy'], label='val_accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.show()
```

## ğŸš€ ìµœì‹  AI íŠ¸ë Œë“œ (2025)

### 1. ìƒì„±í˜• AI (Generative AI)
- **ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ (LLM)**
  - GPT, Claude, Gemini
  - í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§
  - íŒŒì¸íŠœë‹ê³¼ RAG

- **ë©€í‹°ëª¨ë‹¬ AI**
  - í…ìŠ¤íŠ¸ + ì´ë¯¸ì§€ + ìŒì„±
  - DALL-E, Midjourney
  - ë¹„ë””ì˜¤ ìƒì„± AI

### 2. AI ì—ì´ì „íŠ¸ (AI Agents)
```python
# ê°„ë‹¨í•œ AI ì—ì´ì „íŠ¸ ê°œë…
class AIAgent:
    def __init__(self, llm_model):
        self.llm = llm_model
        self.memory = []
        self.tools = []
    
    def process_query(self, query):
        # 1. ì¿¼ë¦¬ ì´í•´
        intent = self.understand_intent(query)
        
        # 2. ë„êµ¬ ì„ íƒ
        tool = self.select_tool(intent)
        
        # 3. ì‹¤í–‰ ë° ì‘ë‹µ
        result = tool.execute(query)
        return result
```

### 3. AI ìµœì í™” ê¸°ìˆ 
- **ëª¨ë¸ ê²½ëŸ‰í™”**: Quantization, Pruning
- **ì—£ì§€ AI**: ëª¨ë°”ì¼, IoT ê¸°ê¸°
- **ì—°í•© í•™ìŠµ**: ë¶„ì‚° í•™ìŠµ íŒ¨ëŸ¬ë‹¤ì„

## ğŸ“Š AI ê°œë°œ í™˜ê²½ ì„¤ì •

### í•„ìˆ˜ ì„¤ì¹˜ ëª©ë¡
```bash
# Python í™˜ê²½ (Anaconda ê¶Œì¥)
conda create -n ai_env python=3.9
conda activate ai_env

# ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬
pip install numpy pandas matplotlib seaborn
pip install scikit-learn

# ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬
pip install tensorflow
pip install torch torchvision

# ì‹œê°í™” ë„êµ¬
pip install plotly
pip install jupyter

# NLP ë„êµ¬
pip install transformers
pip install spacy
```

### ê°œë°œ ë„êµ¬
- **IDE**: VS Code, PyCharm, Jupyter Notebook
- **í´ë¼ìš°ë“œ í”Œë«í¼**: Google Colab, Kaggle, AWS SageMaker
- **ë²„ì „ ê´€ë¦¬**: Git, DVC (Data Version Control)
- **ì‹¤í—˜ ê´€ë¦¬**: MLflow, Weights & Biases

## ğŸ¯ ì»¤ë¦¬ì–´ ê²½ë¡œ

### 1. ë°ì´í„° ì‚¬ì´ì–¸í‹°ìŠ¤íŠ¸
- **í•„ìš” ìŠ¤í‚¬**: í†µê³„, SQL, ì‹œê°í™”
- **í‰ê·  ì—°ë´‰**: $150,000/ë…„
- **ì£¼ìš” ì—…ë¬´**: ë°ì´í„° ë¶„ì„, ì¸ì‚¬ì´íŠ¸ ë„ì¶œ

### 2. ë¨¸ì‹ ëŸ¬ë‹ ì—”ì§€ë‹ˆì–´
- **í•„ìš” ìŠ¤í‚¬**: MLOps, í´ë¼ìš°ë“œ, ì‹œìŠ¤í…œ ì„¤ê³„
- **í‰ê·  ì—°ë´‰**: $135,000/ë…„
- **ì£¼ìš” ì—…ë¬´**: ëª¨ë¸ ë°°í¬, íŒŒì´í”„ë¼ì¸ êµ¬ì¶•

### 3. AI ì—°êµ¬ì›
- **í•„ìš” ìŠ¤í‚¬**: ìˆ˜í•™, ë…¼ë¬¸ ì‘ì„±, ì•Œê³ ë¦¬ì¦˜
- **í‰ê·  ì—°ë´‰**: $160,000/ë…„
- **ì£¼ìš” ì—…ë¬´**: ìƒˆë¡œìš´ ì•Œê³ ë¦¬ì¦˜ ê°œë°œ

## ğŸ” ì‹¬í™” í•™ìŠµ ë°©í–¥

### ë‹¨ê¸° ëª©í‘œ (1ì£¼ì¼)
- Python ê¸°ë°˜ ë¨¸ì‹ ëŸ¬ë‹ í”„ë¡œì íŠ¸ 3ê°œ ì™„ì„±
- Kaggle ê²½ì§„ëŒ€íšŒ ì°¸ì—¬
- TensorFlow/PyTorch ê³µì‹ íŠœí† ë¦¬ì–¼ ì™„ì£¼

### ì¤‘ê¸° ëª©í‘œ (1ê°œì›”)
- ì»´í“¨í„° ë¹„ì „ ë˜ëŠ” NLP íŠ¹í™” í•™ìŠµ
- ì‹¤ì œ ë°ì´í„°ì…‹ìœ¼ë¡œ end-to-end í”„ë¡œì íŠ¸
- AI ë…¼ë¬¸ ì½ê¸° ì‹œì‘

### ì¥ê¸° ëª©í‘œ (3ê°œì›”)
- ì „ë¬¸ ë¶„ì•¼ ì„ íƒí•˜ì—¬ ê¹Šì´ ìˆëŠ” í•™ìŠµ
- ì˜¤í”ˆì†ŒìŠ¤ ê¸°ì—¬ ë˜ëŠ” ë¸”ë¡œê·¸ ìš´ì˜
- AI ì»¤ë®¤ë‹ˆí‹° í™œë™ ë° ë„¤íŠ¸ì›Œí‚¹

## ğŸ’¡ í•™ìŠµ ë¦¬ì†ŒìŠ¤

### ì˜¨ë¼ì¸ ê°•ì˜
- **Coursera**: Andrew Ngì˜ Machine Learning Course
- **Fast.ai**: Practical Deep Learning for Coders
- **Udacity**: AI Nanodegree

### ì‹¤ìŠµ í”Œë«í¼
- **Kaggle**: ë°ì´í„° ê³¼í•™ ê²½ì§„ëŒ€íšŒ
- **Papers With Code**: ìµœì‹  ì—°êµ¬ ë…¼ë¬¸ê³¼ ì½”ë“œ
- **Google AI Education**: ë¬´ë£Œ AI êµìœ¡ ìë£Œ

### ë„ì„œ ì¶”ì²œ
- "íŒŒì´ì¬ ë¨¸ì‹ ëŸ¬ë‹ ì™„ë²½ ê°€ì´ë“œ" - ê¶Œì² ë¯¼
- "í•¸ì¦ˆì˜¨ ë¨¸ì‹ ëŸ¬ë‹" - ì˜¤ë ë¦¬ì•™ ì œë¡±
- "ë”¥ëŸ¬ë‹" - ì´ì•ˆ êµ¿í ë¡œìš°

## ğŸ“ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] AIì˜ ê¸°ë³¸ ê°œë…ê³¼ ë¶„ì•¼ ì´í•´
- [ ] ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ 5ê°œ ì´ìƒ í•™ìŠµ
- [ ] ì‹ ê²½ë§ê³¼ ë”¥ëŸ¬ë‹ êµ¬ì¡° íŒŒì•…
- [ ] TensorFlow ë˜ëŠ” PyTorch ê¸°ë³¸ ì‚¬ìš©ë²•
- [ ] MNIST ì†ê¸€ì”¨ ì¸ì‹ í”„ë¡œì íŠ¸ ì™„ì„±
- [ ] ê°œë°œ í™˜ê²½ ì„¤ì • ì™„ë£Œ
- [ ] í–¥í›„ í•™ìŠµ ê³„íš ìˆ˜ë¦½

---

ğŸ’¡ **ì„±ê³µ íŒ**: AI í•™ìŠµì€ ì´ë¡ ê³¼ ì‹¤ìŠµì˜ ê· í˜•ì´ ì¤‘ìš”í•©ë‹ˆë‹¤. ë§¤ì¼ ì¡°ê¸ˆì”©ì´ë¼ë„ ì½”ë”©í•˜ê³ , ì‹¤ì œ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” í”„ë¡œì íŠ¸ë¥¼ ì§„í–‰í•˜ì„¸ìš”. ì»¤ë®¤ë‹ˆí‹°ì— ì°¸ì—¬í•˜ì—¬ ë‹¤ë¥¸ í•™ìŠµìë“¤ê³¼ ê²½í—˜ì„ ê³µìœ í•˜ëŠ” ê²ƒë„ í° ë„ì›€ì´ ë©ë‹ˆë‹¤!