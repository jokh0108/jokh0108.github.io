# ìì—°ì–´ì²˜ë¦¬ í•™ìŠµ ë¡œë“œë§µ (1ì¼ ì†ì„± ê³¼ì •)

## ğŸ“š ì¶œì²˜
- [Hugging Face NLP Course](https://huggingface.co/course/chapter1/1)
- [Stanford CS224N NLP Course](http://web.stanford.edu/class/cs224n/)
- [Spacy NLP Tutorial](https://spacy.io/usage/spacy-101)
- [NLTK Documentation](https://www.nltk.org/)

## ğŸ¯ í•™ìŠµ ëª©í‘œ
- ìì—°ì–´ì²˜ë¦¬ì˜ ê¸°ë³¸ ê°œë…ê³¼ íŒŒì´í”„ë¼ì¸ ì´í•´
- í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ì™€ í† í°í™” ê¸°ë²• ìŠµë“
- íŠ¸ëœìŠ¤í¬ë¨¸ì™€ BERT ëª¨ë¸ í™œìš©
- ì‹¤ë¬´ì—ì„œ ì‚¬ìš©í•˜ëŠ” NLP ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©ë²•

## ğŸ“‹ 1ì¼ í•™ìŠµ ê³„íš (8ì‹œê°„)

### 1ë‹¨ê³„: NLP ê¸°ì´ˆ (2ì‹œê°„)
#### 1.1 ìì—°ì–´ì²˜ë¦¬ ê°œìš” (30ë¶„)
- NLPì˜ ì •ì˜ì™€ ì‘ìš© ë¶„ì•¼
- ì–¸ì–´ì˜ ê³„ì¸µ êµ¬ì¡° (ìŒì„±í•™, í˜•íƒœë¡ , êµ¬ë¬¸ë¡ , ì˜ë¯¸ë¡ )
- NLPì˜ ì£¼ìš” ê³¼ì œë“¤

#### 1.2 í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ (1ì‹œê°„ 30ë¶„)
```python
import nltk
import spacy
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer, WordNetLemmatizer

# ê¸°ë³¸ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
def preprocess_text(text):
    # 1. í† í°í™”
    tokens = word_tokenize(text.lower())
    
    # 2. ë¶ˆìš©ì–´ ì œê±°
    stop_words = set(stopwords.words('english'))
    tokens = [token for token in tokens if token not in stop_words]
    
    # 3. ì–´ê°„ ì¶”ì¶œ (Stemming)
    stemmer = PorterStemmer()
    stemmed = [stemmer.stem(token) for token in tokens]
    
    # 4. í‘œì œì–´ ì¶”ì¶œ (Lemmatization)
    lemmatizer = WordNetLemmatizer()
    lemmatized = [lemmatizer.lemmatize(token) for token in tokens]
    
    return lemmatized
```

### 2ë‹¨ê³„: ì–¸ì–´ ëª¨ë¸ê³¼ ë²¡í„°í™” (2ì‹œê°„)
#### 2.1 í…ìŠ¤íŠ¸ ë²¡í„°í™” ê¸°ë²• (1ì‹œê°„)
```python
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from gensim.models import Word2Vec, FastText
import numpy as np

# TF-IDF ë²¡í„°í™”
vectorizer = TfidfVectorizer(max_features=1000)
tfidf_matrix = vectorizer.fit_transform(documents)

# Word2Vec ì„ë² ë”©
sentences = [text.split() for text in documents]
w2v_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1)

# ë¬¸ì„œ ì„ë² ë”© ìƒì„±
def doc_embedding(text, model):
    words = text.split()
    vectors = [model.wv[word] for word in words if word in model.wv]
    return np.mean(vectors, axis=0) if vectors else np.zeros(100)
```

#### 2.2 ì–¸ì–´ ëª¨ë¸ ê¸°ì´ˆ (1ì‹œê°„)
- N-gram ëª¨ë¸
- í†µê³„ì  ì–¸ì–´ ëª¨ë¸
- ì‹ ê²½ë§ ì–¸ì–´ ëª¨ë¸ (RNN, LSTM)

### 3ë‹¨ê³„: íŠ¸ëœìŠ¤í¬ë¨¸ì™€ BERT (2ì‹œê°„)
#### 3.1 íŠ¸ëœìŠ¤í¬ë¨¸ ì•„í‚¤í…ì²˜ (1ì‹œê°„)
```python
from transformers import AutoTokenizer, AutoModel
import torch

# BERT ëª¨ë¸ ë¡œë“œ
tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')
model = AutoModel.from_pretrained('bert-base-uncased')

# í…ìŠ¤íŠ¸ ì¸ì½”ë”©
def encode_text(text):
    inputs = tokenizer(text, return_tensors='pt', 
                      padding=True, truncation=True, max_length=512)
    
    with torch.no_grad():
        outputs = model(**inputs)
        embeddings = outputs.last_hidden_state.mean(dim=1)
    
    return embeddings
```

#### 3.2 Pre-trained ëª¨ë¸ í™œìš© (1ì‹œê°„)
```python
from transformers import pipeline

# ê°ì • ë¶„ì„
sentiment_pipeline = pipeline("sentiment-analysis")
result = sentiment_pipeline("I love this product!")

# ì§ˆì˜ì‘ë‹µ
qa_pipeline = pipeline("question-answering")
context = "The quick brown fox jumps over the lazy dog."
question = "What animal jumps?"
answer = qa_pipeline(question=question, context=context)

# í…ìŠ¤íŠ¸ ìš”ì•½
summarizer = pipeline("summarization")
summary = summarizer("Long text to summarize...", max_length=50)
```

### 4ë‹¨ê³„: NLP ì‘ìš©ê³¼ ì‹¤ìŠµ (2ì‹œê°„)
#### 4.1 í…ìŠ¤íŠ¸ ë¶„ë¥˜ í”„ë¡œì íŠ¸ (1ì‹œê°„)
```python
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report

# ë°ì´í„° ì¤€ë¹„ (ì˜ˆ: ìŠ¤íŒ¸ ë¶„ë¥˜)
X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.2)

# ë²¡í„°í™”
vectorizer = TfidfVectorizer(max_features=5000)
X_train_tfidf = vectorizer.fit_transform(X_train)
X_test_tfidf = vectorizer.transform(X_test)

# ëª¨ë¸ í›ˆë ¨
classifier = MultinomialNB()
classifier.fit(X_train_tfidf, y_train)

# ì˜ˆì¸¡ ë° í‰ê°€
predictions = classifier.predict(X_test_tfidf)
print(classification_report(y_test, predictions))
```

#### 4.2 ê³ ê¸‰ NLP íƒœìŠ¤í¬ (1ì‹œê°„)
```python
import spacy

# Named Entity Recognition (NER)
nlp = spacy.load("en_core_web_sm")
doc = nlp("Apple Inc. was founded by Steve Jobs in Cupertino.")
for ent in doc.ents:
    print(f"{ent.text} - {ent.label_}")

# ì˜ì¡´ì„± íŒŒì‹±
for token in doc:
    print(f"{token.text} -> {token.dep_} -> {token.head.text}")

# í’ˆì‚¬ íƒœê¹…
for token in doc:
    print(f"{token.text} - {token.pos_}")
```

## ğŸ”§ ì£¼ìš” NLP ë¼ì´ë¸ŒëŸ¬ë¦¬

### 1. NLTK (ìì—°ì–´ íˆ´í‚·)
```python
import nltk
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
```

### 2. spaCy (ì‚°ì—…ìš© NLP)
```python
import spacy
nlp = spacy.load("en_core_web_sm")
```

### 3. Hugging Face Transformers
```python
from transformers import pipeline, AutoTokenizer, AutoModel
```

### 4. Gensim (í† í”½ ëª¨ë¸ë§)
```python
from gensim.models import LdaModel, Word2Vec, Doc2Vec
```

## ğŸ“Š NLP í‰ê°€ ì§€í‘œ

### ë¶„ë¥˜ íƒœìŠ¤í¬
- ì •í™•ë„ (Accuracy)
- ì •ë°€ë„ (Precision)
- ì¬í˜„ìœ¨ (Recall)
- F1-Score

### ìƒì„± íƒœìŠ¤í¬
- BLEU Score (ê¸°ê³„ë²ˆì—­)
- ROUGE Score (ìš”ì•½)
- Perplexity (ì–¸ì–´ëª¨ë¸)

## ğŸš€ ìµœì‹  NLP íŠ¸ë Œë“œ

### 1. ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ (LLM)
- GPT ì‹œë¦¬ì¦ˆ
- BERT, RoBERTa
- T5, BART

### 2. í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§
```python
# Few-shot learning ì˜ˆì œ
prompt = """
Classify the sentiment of the following texts:

Text: "I love this movie!"
Sentiment: Positive

Text: "This is terrible."
Sentiment: Negative

Text: "The weather is okay."
Sentiment: 
"""
```

### 3. ë©€í‹°ëª¨ë‹¬ NLP
- í…ìŠ¤íŠ¸ + ì´ë¯¸ì§€
- ìŒì„± + í…ìŠ¤íŠ¸
- ë¹„ë””ì˜¤ + í…ìŠ¤íŠ¸

## ğŸ¯ ì‹¤ë¬´ í”„ë¡œì íŠ¸ ì•„ì´ë””ì–´

1. **ì±—ë´‡ ê°œë°œ**
2. **ê°ì • ë¶„ì„ ëŒ€ì‹œë³´ë“œ**
3. **ë‰´ìŠ¤ ìš”ì•½ ì‹œìŠ¤í…œ**
4. **í‚¤ì›Œë“œ ì¶”ì¶œ ë„êµ¬**
5. **ì–¸ì–´ ë²ˆì—­ê¸°**

## ğŸ“š ì¶”ì²œ ìë£Œ

### ë„ì„œ
- "ìì—°ì–´ ì²˜ë¦¬ ì¿¡ë¶" - ì•¨ë¦¬ìŠ¤ ì •
- "ë°‘ë°”ë‹¥ë¶€í„° ì‹œì‘í•˜ëŠ” ë”¥ëŸ¬ë‹ 2" - ì‚¬ì´í†  ê³ í‚¤

### ì˜¨ë¼ì¸ ê°•ì˜
- CS224N: Natural Language Processing with Deep Learning
- Hugging Face NLP Course

## ğŸ“ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ êµ¬í˜„
- [ ] TF-IDFì™€ Word2Vec ì°¨ì´ ì´í•´
- [ ] BERT ëª¨ë¸ í™œìš© ê°€ëŠ¥
- [ ] í…ìŠ¤íŠ¸ ë¶„ë¥˜ í”„ë¡œì íŠ¸ ì™„ì„±
- [ ] NERê³¼ í’ˆì‚¬ íƒœê¹… êµ¬í˜„
- [ ] ì‹¤ë¬´ í”„ë¡œì íŠ¸ 1ê°œ ê¸°íš

---

ğŸ’¡ **í•™ìŠµ íŒ**: NLPëŠ” ë‹¤ì–‘í•œ ì–¸ì–´ì™€ ë„ë©”ì¸ì— ë”°ë¼ ì„±ëŠ¥ì´ ë‹¬ë¼ì§‘ë‹ˆë‹¤. ì‹¤ì œ ë°ì´í„°ë¡œ ë§ì´ ì‹¤í—˜í•´ë³´ê³ , ìµœì‹  ëª¨ë¸ë“¤ì˜ ë™í–¥ì„ ê¾¸ì¤€íˆ ë”°ë¼ê°€ì„¸ìš”!